# -*- coding: utf-8 -*-
"""MultiAgent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z7TXQzrNiY9Ltzadzd9EVe2lGac-BbtE
"""

!pip install openai timm supervision yapf einops gradio datasets addict lightning pytorch_lightning git+https://github.com/openai/CLIP.git huggingface_hub

from google.colab import drive
drive.mount('/content/drive')

groundingdino_checkpoint_path = '/content/drive/My Drive/groundingdino_swint_ogc.pth'

import torch
from datasets import load_dataset
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from transformers import Idefics2ForConditionalGeneration, AutoProcessor
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
from lightning.pytorch.loggers import WandbLogger
import lightning as L

# Grounded SAM and CLIP Count imports
from inference import load_model
from automatic_mask_generator import SamAutomaticMaskGenerator
from run import Model as CLIP_Count
from query_vlm import QueryVLM
from query_llm import QueryLLM
from sam import Sam
from groundingdino import GroundingDINO

MAX_LENGTH = 1024

class Idefics2Dataset(Dataset):
    def __init__(self, dataset):
        super().__init__()
        self.dataset = dataset

    def __len__(self) -> int:
        return len(self.dataset)

    def __getitem__(self, idx: int) -> tuple:
        samples = self.dataset
        question = samples['question'][idx]
        options = samples['options'][idx]
        answer = samples["answer"][idx]
        image = samples["image"][idx]
        return question, options, answer, image

dataset = load_dataset('lmms-lab/ai2d')
split_dataset = dataset['test'].train_test_split(test_size=0.1)
train_dataset = Idefics2Dataset(split_dataset['train'])
validation_dataset = Idefics2Dataset(split_dataset['test'][:10])

checkpoint = "HuggingFaceM4/idefics2-8b"
model = Idefics2ForConditionalGeneration.from_pretrained(
    checkpoint,
    torch_dtype=torch.bfloat16,
    device_map=None,
)
device = torch.device("cuda")
model.to(device)
processor = AutoProcessor.from_pretrained(checkpoint)
image_token_id = processor.tokenizer.additional_special_tokens_ids[processor.tokenizer.additional_special_tokens.index("<image>")]

def train_collate_fn(examples):
    texts = []
    images = []
    for example in examples:
        question, options, answer, image_example = example
        question += 'Based on the given image, please answer the question, the answer should be a number. \
        0 represents the first option. 1 represents the second option. 2 represents the third option. \
        3 represents the fourth option. Please choose from the following options using a number above: '
        content = [{"type": "image"}]
        content += [{"type": "text", "text": question}]
        content += [{"type": "text", "text": options}]

        # Create inputs
        messages = [
            {
                "role": "user",
                "content": content,
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": answer},
                ]
            },
        ]

        prompt = processor.apply_chat_template(messages, add_generation_prompt=False)
        texts.append(prompt)
        images.append(image_example)

    batch = processor(text=texts, images=images, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")

    labels = batch["input_ids"].clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100
    labels[labels == model.config.image_token_id] = -100
    batch["labels"] = labels

    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    pixel_values = batch["pixel_values"]
    pixel_attention_mask = batch["pixel_attention_mask"]
    labels = batch["labels"]

   
    input_ids = input_ids.to(torch.long)  # long 
    labels = labels.to(torch.long)        # long 
    attention_mask = attention_mask.to(torch.bfloat16)
    pixel_values = pixel_values.to(torch.bfloat16)
    pixel_attention_mask = pixel_attention_mask.to(torch.bfloat16)

    return input_ids, attention_mask, pixel_values, pixel_attention_mask, labels



def eval_collate_fn(examples):
    images = []
    texts = []  # Prompts
    answers = []
    for example in examples:
        question, options, answer, image_example = example
        question += 'Based on the given image, please answer the question, the answer should be a number. \
                0 represents the first option. 1 represents the second option. 2 represents the third option. \
                3 represents the fourth option. Please choose from the following options using a number above: '
        content = [{"type": "image"}]
        content += [{"type": "text", "text": question}]
        content += [{"type": "text", "text": options}]

        messages = [
            {
                "role": "user",
                "content": content,
            },
        ]
        text = processor.apply_chat_template(messages, add_generation_prompt=True)
        images.append(image_example)
        texts.append(text.strip())
        answers.append(answer)

    batch = processor(text=texts, images=images, return_tensors="pt", padding=True)

    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    pixel_values = batch["pixel_values"]
    pixel_attention_mask = batch["pixel_attention_mask"]

    # Make sure all tensors are converted to the same data type as the model
    input_ids = input_ids.to(torch.long)  # long 
    attention_mask = attention_mask.to(torch.bfloat16)
    pixel_values = pixel_values.to(torch.bfloat16)
    pixel_attention_mask = pixel_attention_mask.to(torch.bfloat16)

    return input_ids, attention_mask, pixel_values, pixel_attention_mask, answers

# Ensure correct initialization of the GroundingDINO model
class GroundingDINO:
    def __init__(self, config_path, checkpoint_path, num_queries=100):
        self.config_path = config_path
        self.checkpoint_path = checkpoint_path
        self.num_queries = num_queries
        self.model = self.load_model()

    def load_model(self):
        # Actual implementation of loading model configuration and weights
        return None  # Implementing model loading logic

    def predict(self, image):
        # Practical implementation of processing images
        return None  # Return prediction results

class MultiAgentInference(L.LightningModule):
    def __init__(self, config, processor, model):
        super().__init__()
        self.config = config
        self.processor = processor
        self.model = model
        self.clip_count = self.load_clip_count(config.get('clip_count', None))
        self.grounding_dino = self.load_grounding_dino(config['dino'])
        self.batch_size = config.get("batch_size")

    def load_grounding_dino(self, dino_config):
        config_path = dino_config['GROUNDING_DINO_CONFIG_PATH']
        checkpoint_path = dino_config['GROUNDING_DINO_CHECKPOINT_PATH']
        grounding_dino_model = GroundingDINO(config_path, checkpoint_path, num_queries=100)
        return grounding_dino_model

    def load_clip_count(self, clip_count_config):
        if clip_count_config:
            clip_count_model = CLIP_Count()
        else:
            print("No clip_count configuration provided, skipping CLIP Count model loading.")
            clip_count_model = None
        return clip_count_model

    def training_step(self, batch, batch_idx):
        input_ids, attention_mask, pixel_values, pixel_attention_mask, labels = batch
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, labels=labels)
        loss = outputs.loss
        self.log('train_loss', loss)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids, attention_mask, pixel_values, pixel_attention_mask, answers = batch
        outputs = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, max_length=MAX_LENGTH, num_beams=4)
        predictions = self.processor.batch_decode(outputs, skip_special_tokens=True)
        accuracy = self._calculate_accuracy(predictions, answers)
        self.log('val_accuracy', accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        return accuracy

    def _calculate_accuracy(self, predictions, answers):
        total = 0
        correct = 0
        for pred, answer in zip(predictions, answers):
            if pred == answer:
                correct += 1
            total += 1
        return correct / total

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config['lr'])
        return optimizer

    def train_dataloader(self):
        return DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=self.batch_size, shuffle=True)

    def val_dataloader(self):
        return DataLoader(validation_dataset, collate_fn=eval_collate_fn, batch_size=self.batch_size, shuffle=False)

torch.cuda.empty_cache()

# Configuration dictionary
config = {
    "max_epochs": 2,
    "check_val_every_n_epoch": 1,
    "gradient_clip_val": 1.0,
    "accumulate_grad_batches": 8,
    "lr": 1e-4,
    "batch_size": 1,
    "precision": "16-mixed",
    "verbose": True,
    "dino": {
        "GROUNDING_DINO_CONFIG_PATH": "GroundingDINO_SwinT_OGC.py",
        "GROUNDING_DINO_CHECKPOINT_PATH": groundingdino_checkpoint_path
    },
}


# Early stopping callback
early_stop_callback = EarlyStopping(monitor="val_loss", patience=3, mode='min')

# Initialize the MultiAgentInference class with the processor, model, and grounding_dino
multi_agent_inference = MultiAgentInference(config, processor, model)

# Trainer initialization
trainer = L.Trainer(
    accelerator="gpu",
    devices=1,
    max_epochs=config.get("max_epochs"),
    check_val_every_n_epoch=config.get("check_val_every_n_epoch"),
    gradient_clip_val=config.get("gradient_clip_val"),
    accumulate_grad_batches=config.get("accumulate_grad_batches"),
    precision=config.get("precision"),
    num_sanity_val_steps=0,
    callbacks=early_stop_callback
)

# Fit the model
trainer.fit(multi_agent_inference)

# Prepare the validation data loader
validation_loader = DataLoader(validation_dataset, collate_fn=eval_collate_fn, batch_size=1, shuffle=False)

# Perform inference
results, accuracy = multi_agent_inference.inference(validation_loader)

# Print accuracy and results
print(f"Accuracy: {accuracy * 100:.2f}%")
for pred, answer in results:
    print(f"Prediction: {pred}, Answer: {answer}")

# Handle the case where CLIP Count model configuration is missing
print("No clip_count configuration provided, skipping CLIP Count model loading.")